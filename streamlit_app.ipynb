{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlZnFPCm3QGJAliDAs3UwV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gsowndarya/ChatGPT-NLP/blob/main/streamlit_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "6jSU9sRSzEyC",
        "outputId": "550ceaf5-9b3f-4a66-eef3-889a0bd15b4f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-efbee0a8-e9e4-406f-8bba-8da84905bc43\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-efbee0a8-e9e4-406f-8bba-8da84905bc43\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving best_model.h5 to best_model.h5\n",
            "Saving cleaned_reviews.csv to cleaned_reviews.csv\n",
            "Saving tokenizer.pkl to tokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok==5.2.1 streamlit\n"
      ],
      "metadata": {
        "id": "Msj15H7UbUKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47e2e32-3c5d-4973-c613-56b88dfdb306"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==5.2.1\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/761.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m593.9/761.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m761.3/761.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyngrok==5.2.1) (6.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.39.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19772 sha256=de7593f4022b115ae094112f9bc7d25da2375713bf3538683477d75c24471ca9\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/70/ba/c8ce0a13cae74dc3a49d7c4155691aa96ce9c2af5c70e809aa\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: watchdog, pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-5.2.1 streamlit-1.45.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment==3.3.2 contractions==0.1.73"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtRNyy2CmiSq",
        "outputId": "265ea50c-5f0a-4255-8443-729e73d67feb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment==3.3.2\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Collecting contractions==0.1.73\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment==3.3.2) (2.32.3)\n",
            "Collecting textsearch>=0.0.21 (from contractions==0.1.73)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions==0.1.73)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions==0.1.73)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment==3.3.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment==3.3.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment==3.3.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment==3.3.2) (2025.4.26)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, vaderSentiment, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24 vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH3b3Gkn92XT",
        "outputId": "71adcff8-c6a1-4e87-da71-a6e5b444e7f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocpp9jpt990K",
        "outputId": "98dab0ed-ca24-436d-c95d-4a5b9a54f1e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.text import Tokenizer\n",
        "#from keras.models import load_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Download NLTK stopwords if not already\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set Streamlit page config\n",
        "st.set_page_config(page_title=\"ChatGPT Sentiment Dashboard & Analyzer\", layout=\"wide\")\n",
        "\n",
        "# Title\n",
        "st.title(\"üìä ChatGPT Review Sentiment Dashboard & Analyzer\")\n",
        "\n",
        "# Load tokenizer and model for sentiment analyzer\n",
        "with open('tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "max_len = 100\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>+', '', text)\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    cleaned = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([cleaned])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "    pred = model.predict(padded)\n",
        "    label = np.argmax(pred, axis=1)[0]\n",
        "    label_map = {0: 'Negative üòû', 1: 'Neutral üòê', 2: 'Positive üòä'}\n",
        "    return label_map[label]\n",
        "\n",
        "def show_colored_sentiment(label):\n",
        "    colors = {\n",
        "        'Negative üòû': 'red',\n",
        "        'Neutral üòê': 'orange',\n",
        "        'Positive üòä': 'green'\n",
        "    }\n",
        "    color = colors.get(label, 'black')\n",
        "    st.markdown(f\"<h3 style='color:{color}'>Predicted Sentiment: <b>{label}</b></h3>\", unsafe_allow_html=True)\n",
        "\n",
        "# Sidebar menu\n",
        "st.sidebar.title(\"Menu\")\n",
        "section = st.sidebar.radio(\"Choose Section\", [\"üí¨ Sentiment Analyzer\", \"üìà Data Visualizations\"])\n",
        "\n",
        "if section == \"üí¨ Sentiment Analyzer\":\n",
        "    st.header(\"üß™ ChatGPT Feedback Analysis\")\n",
        "    user_input = st.text_area(\"‚úçÔ∏èEnter a review about ChatGPT and instantly discover whether it‚Äôs Positive, Neutral, or Negative!\")\n",
        "\n",
        "    if st.button(\"Analyze\"):\n",
        "        if user_input.strip() == \"\":\n",
        "            st.warning(\"Please enter a review.\")\n",
        "        else:\n",
        "            sentiment = predict_sentiment(user_input)\n",
        "            show_colored_sentiment(sentiment)\n",
        "\n",
        "elif section == \"üìà Data Visualizations\":\n",
        "    st.header(\"üìà ChatGPT Review Sentiment Dashboard\")\n",
        "\n",
        "    # Load pre-cleaned CSV file - update the path if needed\n",
        "    csv_file_path = \"cleaned_reviews.csv\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading file: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "    # Numbered visualization questions dictionary\n",
        "    question_options = {\n",
        "        \"Q1. Overall Sentiment of User Reviews\": \"sentiment_distribution\",\n",
        "        \"Q2. Sentiment Variation by Rating\": \"sentiment_by_rating\",\n",
        "        \"Q3. Keywords by Sentiment\": \"sentiment_keywords\",\n",
        "        \"Q4. Sentiment Trend Over Time (Monthly)\": \"sentiment_over_time\",\n",
        "        \"Q5. Sentiment by Verified Purchase\": \"verified_sentiment\",\n",
        "        \"Q6. Review Length vs Rating\": \"review_length_vs_rating\",\n",
        "        \"Q7. Which locations show the most Positive or Negative Sentiment?\": \"location_sentiment\",\n",
        "        \"Q8. Sentiment by Platform\": \"platform_sentiment\",\n",
        "        \"Q9. Sentiment by ChatGPT Version\": \"version_sentiment\",\n",
        "        \"Q10. Common Negative Feedback Themes\": \"negative_feedback_themes\"\n",
        "    }\n",
        "\n",
        "    selected_question = st.selectbox(\"Select a question to visualize\", list(question_options.keys()))\n",
        "    selected_viz = question_options[selected_question]\n",
        "\n",
        "    if selected_viz == \"sentiment_distribution\":\n",
        "      st.subheader(\"Q1. Overall Sentiment of User Reviews\")\n",
        "\n",
        "      sentiment_counts = df['sentiment'].value_counts()\n",
        "\n",
        "      # Show count table only\n",
        "      st.table(pd.DataFrame({'Count': sentiment_counts}))\n",
        "\n",
        "      # Bar chart with Plotly\n",
        "      fig_bar = px.bar(\n",
        "          x=sentiment_counts.index,\n",
        "          y=sentiment_counts.values,\n",
        "          labels={'x': 'Sentiment', 'y': 'Count'},\n",
        "          title='Sentiment Counts Bar Chart',\n",
        "          color=sentiment_counts.index,\n",
        "          color_discrete_map={'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'})\n",
        "      st.plotly_chart(fig_bar)\n",
        "\n",
        "      # Pie chart with Plotly\n",
        "      fig_pie = px.pie(\n",
        "          values=sentiment_counts.values,\n",
        "          names=sentiment_counts.index,\n",
        "          title='Sentiment Distribution Pie Chart',\n",
        "          color=sentiment_counts.index,\n",
        "          color_discrete_map={'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'})\n",
        "      st.plotly_chart(fig_pie)\n",
        "\n",
        "\n",
        "    elif selected_viz == \"sentiment_by_rating\":\n",
        "      st.subheader(\"Q2. Sentiment Variation by Rating\")\n",
        "\n",
        "      # Sentiment distribution by rating\n",
        "      sentiment_rating_counts = df.groupby(['rating', 'sentiment']).size().reset_index(name='count')\n",
        "\n",
        "      fig = px.bar(\n",
        "          sentiment_rating_counts,\n",
        "          x='rating',\n",
        "          y='count',\n",
        "          color='sentiment',\n",
        "          title='Sentiment Distribution by Rating',\n",
        "          labels={'count': 'Number of Reviews', 'rating': 'Rating', 'sentiment': 'Sentiment'},\n",
        "          color_discrete_map={'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'},\n",
        "          barmode='stack')\n",
        "      st.plotly_chart(fig)\n",
        "\n",
        "      # Define expected sentiment by rating\n",
        "      def is_mismatch(row):\n",
        "        if row['rating'] == 5 and row['sentiment'] != 'Positive':\n",
        "          return True\n",
        "        elif row['rating'] == 4 and row['sentiment'] not in ['Positive', 'Neutral']:\n",
        "          return True\n",
        "        elif row['rating'] == 3 and row['sentiment'] not in ['Positive', 'Neutral']:\n",
        "          return True\n",
        "        elif row['rating'] == 2 and row['sentiment'] not in ['Negative', 'Neutral']:\n",
        "          return True\n",
        "        elif row['rating'] == 1 and row['sentiment'] != 'Negative':\n",
        "          return True\n",
        "        return False\n",
        "\n",
        "\n",
        "      # Apply mismatch filter\n",
        "      mismatch = df[df.apply(is_mismatch, axis=1)]\n",
        "\n",
        "      st.write(f\"Number of mismatched reviews (rating vs sentiment): {len(mismatch)}\")\n",
        "\n",
        "      if not mismatch.empty:\n",
        "        st.dataframe(mismatch[['rating', 'sentiment', 'review']].head(10))\n",
        "\n",
        "\n",
        "    elif selected_viz == \"sentiment_keywords\":\n",
        "      st.subheader(\"Q3. Keywords by Sentiment\")\n",
        "\n",
        "      # Word Cloud for each sentiment\n",
        "      sentiments = ['Positive', 'Neutral', 'Negative']\n",
        "      for sent in sentiments:\n",
        "        st.markdown(f\"### {sent} Reviews\")\n",
        "        text = \" \".join(df[df['sentiment'] == sent]['lemmatized_review'])\n",
        "\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        ax.imshow(wordcloud, interpolation='bilinear')\n",
        "        ax.axis(\"off\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # Function to get top words\n",
        "      def get_top_words(sentiment, n=10):\n",
        "        text = \" \".join(df[df['sentiment'] == sentiment]['lemmatized_review'])\n",
        "        tokens = text.split()\n",
        "        most_common = Counter(tokens).most_common(n)\n",
        "        return pd.DataFrame(most_common, columns=['Word', 'Frequency'])\n",
        "\n",
        "      # Display top 10 words as table\n",
        "      st.subheader(\"Top 10 Words by Sentiment\")\n",
        "      for sent in sentiments:\n",
        "        st.markdown(f\"### {sent} Sentiment\")\n",
        "        st.table(get_top_words(sent))\n",
        "\n",
        "\n",
        "    elif selected_viz == \"sentiment_over_time\":\n",
        "        st.subheader(\"Q4. Sentiment Trend Over Time (Monthly)\")\n",
        "\n",
        "        # Ensure 'date' is datetime and extract 'month'\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df['month'] = df['date'].dt.to_period('M').astype(str)\n",
        "\n",
        "        # Group by month and sentiment\n",
        "        monthly_sentiment = df.groupby(['month', 'sentiment']).size().reset_index(name='count')\n",
        "\n",
        "        # Plot\n",
        "        fig = px.line(monthly_sentiment,\n",
        "                      x='month',\n",
        "                      y='count',\n",
        "                      color='sentiment',\n",
        "                      title='Sentiment Trend Over Time',\n",
        "                      markers=True,\n",
        "                      color_discrete_map={\n",
        "                          'Positive': 'green',\n",
        "                          'Neutral': 'orange',\n",
        "                          'Negative': 'red'})\n",
        "\n",
        "        fig.update_layout(xaxis_title='Month', yaxis_title='Number of Reviews', hovermode='x unified')\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "    elif selected_viz == \"verified_sentiment\":\n",
        "        st.subheader(\"Q5. Sentiment by Verified Purchase\")\n",
        "        # Group and count reviews by verified_purchase and sentiment\n",
        "        verified_sentiment = df.groupby(['verified_purchase', 'sentiment']).size().reset_index(name='count')\n",
        "\n",
        "        # Pivot table for display\n",
        "        pivot_df = verified_sentiment.pivot(index='verified_purchase', columns='sentiment', values='count').fillna(0).astype(int)\n",
        "\n",
        "        # Calculate difference row (Yes - No)\n",
        "        diff_row = pivot_df.loc['Yes'] - pivot_df.loc['No']\n",
        "        diff_row.name = 'Difference (Yes - No)'\n",
        "\n",
        "        # Append difference row to pivot_df\n",
        "        pivot_with_diff = pd.concat([pivot_df, diff_row.to_frame().T])\n",
        "\n",
        "        # Bar chart using Plotly\n",
        "        fig = px.bar(\n",
        "            verified_sentiment,\n",
        "            x='verified_purchase',\n",
        "            y='count',\n",
        "            color='sentiment',\n",
        "            barmode='group',\n",
        "            title='Sentiment Distribution by Verified Purchase',\n",
        "            color_discrete_map={\n",
        "                'Positive': 'green',\n",
        "                'Neutral': 'orange',\n",
        "                'Negative': 'red'},\n",
        "            labels={'verified_purchase': 'Verified Purchase','count': 'Number of Reviews'})\n",
        "\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "        # Optional: Display table\n",
        "        st.markdown(\"#### Sentiment Count Table by Verified Purchase\")\n",
        "        st.table(pivot_with_diff)\n",
        "\n",
        "\n",
        "    elif selected_viz == \"review_length_vs_rating\":\n",
        "        st.subheader(\"Q6. Review Length vs Rating\")\n",
        "\n",
        "        # Group by sentiment and calculate average review length\n",
        "        avg_length = df.groupby('sentiment')['review_length'].mean().reset_index()\n",
        "\n",
        "        # Color mapping\n",
        "        color_map = {'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'}\n",
        "        colors = [color_map[sent] for sent in avg_length['sentiment']]\n",
        "\n",
        "        # Plot\n",
        "        fig = px.bar(avg_length,\n",
        "                     x='sentiment',\n",
        "                     y='review_length',\n",
        "                     color='sentiment',\n",
        "                     color_discrete_map=color_map,\n",
        "                     labels={'review_length': 'Average Review Length'},\n",
        "                     title='Average Review Length by Sentiment')\n",
        "\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "    elif selected_viz == \"location_sentiment\":\n",
        "        st.subheader(\"Q7. Which locations show the most Positive or Negative Sentiment?\")\n",
        "        # Group by location and sentiment to count reviews\n",
        "        loc_sentiment = df.groupby(['location', 'sentiment']).size().reset_index(name='count')\n",
        "\n",
        "        # Sort to show top contributing locations (optional)\n",
        "        top_locations = df['location'].value_counts().head(10).index.tolist()\n",
        "        loc_sentiment = loc_sentiment[loc_sentiment['location'].isin(top_locations)]\n",
        "\n",
        "        # Plot\n",
        "        fig = px.bar(loc_sentiment,\n",
        "                     x='location',\n",
        "                     y='count',\n",
        "                     color='sentiment',\n",
        "                     color_discrete_map={'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'},\n",
        "                     title='Sentiment by Location (Top 10)',\n",
        "                     labels={'count': 'Review Count', 'location': 'Location'},\n",
        "                     barmode='group')\n",
        "\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "        sentiment_table = df[df['location'].isin(top_locations)].pivot_table(\n",
        "            index='location',\n",
        "            columns='sentiment',\n",
        "            values='review',\n",
        "            aggfunc='count',\n",
        "            fill_value=0).reset_index()\n",
        "\n",
        "        # Reorder columns for clarity\n",
        "        sentiment_table = sentiment_table[['location', 'Positive', 'Neutral', 'Negative']]\n",
        "\n",
        "        # Show the table\n",
        "        st.dataframe(sentiment_table)\n",
        "\n",
        "\n",
        "    elif selected_viz == \"platform_sentiment\":\n",
        "        st.subheader(\"Q8. Sentiment by Platform\")\n",
        "        # Count of sentiments per platform\n",
        "        platform_sentiment = df.groupby(['platform', 'sentiment']).size().reset_index(name='count')\n",
        "        pivot_platform = platform_sentiment.pivot(index='sentiment', columns='platform', values='count').fillna(0).astype(int)\n",
        "\n",
        "        # Add a row with difference (Positive - Negative) to understand imbalance\n",
        "        pivot_platform['Difference (Mobile - Web)'] = pivot_platform.get('Mobile', 0) - pivot_platform.get('Web', 0)\n",
        "\n",
        "        # Show table\n",
        "        st.dataframe(pivot_platform.reset_index())\n",
        "\n",
        "        # Plot bar chart using Plotly\n",
        "        fig = px.bar(platform_sentiment,\n",
        "                     x='platform',\n",
        "                     y='count',\n",
        "                     color='sentiment',\n",
        "                     barmode='group',\n",
        "                     color_discrete_map={'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'},\n",
        "                     title=\"Sentiment Distribution by Platform\")\n",
        "\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "    elif selected_viz == \"version_sentiment\":\n",
        "        st.subheader(\"Q9. Sentiment by ChatGPT Version\")\n",
        "        # Group by version and sentiment\n",
        "        version_sentiment = df.groupby(['version', 'sentiment']).size().reset_index(name='count')\n",
        "\n",
        "        # Pivot for table view\n",
        "        pivot_version = version_sentiment.pivot(index='version', columns='sentiment', values='count').fillna(0).astype(int)\n",
        "\n",
        "        # Add total reviews and most frequent sentiment per version\n",
        "        pivot_version['Total Reviews'] = pivot_version.sum(axis=1)\n",
        "        pivot_version['Top Sentiment'] = pivot_version[['Positive', 'Neutral', 'Negative']].idxmax(axis=1)\n",
        "\n",
        "        st.dataframe(pivot_version.reset_index())\n",
        "\n",
        "        # Plotly stacked bar chart\n",
        "        fig = px.bar(version_sentiment,\n",
        "                     x='version',\n",
        "                     y='count',\n",
        "                     color='sentiment',\n",
        "                     title='Sentiment Distribution by Version',\n",
        "                     color_discrete_map={'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'})\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "    elif selected_viz == \"negative_feedback_themes\":\n",
        "        st.subheader(\"Q10. Common Negative Feedback Themes\")\n",
        "        # Filter negative reviews\n",
        "        negative_reviews = df[df['sentiment'] == 'Negative']['lemmatized_review']\n",
        "\n",
        "        # Join all reviews and split into tokens\n",
        "        negative_text = \" \".join(negative_reviews).split()\n",
        "\n",
        "        # Get most common keywords\n",
        "        top_negative_words = Counter(negative_text).most_common(20)\n",
        "        top_negative_df = pd.DataFrame(top_negative_words, columns=['Keyword', 'Frequency'])\n",
        "\n",
        "        st.markdown(\"### Top 20 Keywords in Negative Reviews\")\n",
        "        st.table(top_negative_df)\n",
        "\n",
        "        # WordCloud for visualization\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(\" \".join(negative_text))\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        ax.imshow(wordcloud, interpolation='bilinear')\n",
        "        ax.axis(\"off\")\n",
        "        st.pyplot(fig)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M8s3PU_mWwx",
        "outputId": "aef5dbb7-4c2e-4674-91d7-741879668ab4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and install the latest ngrok\n",
        "!wget -qO ngrok.zip https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip -d /usr/local/bin && chmod +x /usr/local/bin/ngrok\n",
        "\n",
        "# Add your ngrok authentication token (replace below with your token)\n",
        "!ngrok config add-authtoken 2xJRPyoirSy9D8YpdP88rm6tWgH_6ocNEUgvkuU6Zyh5xn8NS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZEPsZn88Y2X",
        "outputId": "6a3e385d-fcec-4f02-c5ad-e2c4eb75f05a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok.zip\n",
            "  inflating: /usr/local/bin/ngrok    \n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.headless true\")\n",
        "\n",
        "# Start Streamlit app in a background thread\n",
        "threading.Thread(target=run_streamlit).start()\n",
        "time.sleep(5)  # wait for Streamlit to start\n",
        "\n",
        "# Start ngrok tunnel in background\n",
        "os.system(\"ngrok http 8501 &\")\n",
        "time.sleep(5)  # wait for ngrok to start\n",
        "\n",
        "# Get the public URL\n",
        "try:\n",
        "    tunnels = requests.get(\"http://localhost:4040/api/tunnels\").json()\n",
        "    public_url = tunnels['tunnels'][0]['public_url']\n",
        "    print(\"üåê Your Streamlit app is live here:\")\n",
        "    print(public_url)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error getting ngrok URL:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyJ-aO5L80A0",
        "outputId": "40263106-3a99-4af2-c774-d07f9163010a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Your Streamlit app is live here:\n",
            "https://218d-35-230-94-24.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atiuaXf5pnVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BdW9fYlIs-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1f6G4bmuoK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcJuBBPJuoNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbs8A-x2yAB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9dFc-jnyAJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pickle\n",
        "\n",
        "# Example: create a tokenizer and fit on some texts\n",
        "texts = ['this is a test', 'another test']\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Save tokenizer to file (overwrites old tokenizer.pkl)\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "q_Hl0L1Vx2vc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# Test tokenizer\n",
        "print(tokenizer.texts_to_sequences(['this is a test']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u7NSQQryXgn",
        "outputId": "d9c874fb-03ab-47a3-c544-93d41b546011"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezrPV4uUyXjk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}